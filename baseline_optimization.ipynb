{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization \n",
    "------------\n",
    "*Basic hyperparameter tuning to improve baseline performance*\n",
    "\n",
    "*Group Name: Destiny's Child*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Names\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Miguel Romero Calvo\n",
    "2. Jenny Kong\n",
    "3. Louise Lai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief description\n",
    "---\n",
    "Decision Trees are used to predict the POS tags for each word in a sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "import pprint \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35967 rows of data\n",
      "Here is a sample line:\n",
      "\t\"[16, 16, 9, 16, 15, 16, 16, 31, 16, 9, 7, 16, 35, 7, 15, 28, 30, 16, 2]\",Joint Chiefs of Staff Chairman Mike Mullen said Sunday that the U.S. has a plan to strike Iran .\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "with open('/Users/siangk/Desktop/msds621/mlproject/ML1_final_project/databunch/train.csv') as train_file:\n",
    "    y = train_file.read().splitlines()\n",
    "    \n",
    "print(\"There are {} rows of data\".format(len(y)))\n",
    "print(\"Here is a sample line:\\n\\t{}\".format(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', ',', '.', ':', ';', 'CC', 'CD', 'DT', 'EX', 'IN', 'JJ', 'JJR', 'JJS', 'LRB', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'RRB', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/siangk/Desktop/msds621/mlproject/ML1_final_project/databunch/classes.txt') as classes_file:\n",
    "    classes = classes_file.read().splitlines()\n",
    "\n",
    "print(list(classes))\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit scikit-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    " \n",
    "# pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2)) # example\n",
    "\n",
    "def transformToTuples(trainFile, classesFile):\n",
    "    classes # this should exist in global\n",
    "    taggedSentences = []\n",
    "    \n",
    "    # data cleaning\n",
    "    cleanedTrainFile = trainFile.copy()\n",
    "    del cleanedTrainFile[7561] # delete weird lines e.g.: [7],The\n",
    "    del cleanedTrainFile[10422] # [3],...\n",
    "    \n",
    "    i = 0\n",
    "    for i, line in enumerate(cleanedTrainFile):\n",
    "        splitLine = line.split(\"\\\",\")\n",
    "        \n",
    "        # extract the POS embeddings [0]\n",
    "        tagsString = splitLine[0].replace(\"\\\"\", \"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        tags = tagsString.split(\", \")\n",
    "        \n",
    "        # extract the sentence [1]\n",
    "        sentence = splitLine[1].strip(\"\\\"\") # from 2nd elem to second last, to remove the extra \"\n",
    "        \n",
    "        oneLineTagged = []\n",
    "        for tag, word in zip(tags, sentence.strip().split(\" \")):\n",
    "            #print(int(tag))\n",
    "            posTag = classes[int(tag)-1] \n",
    "            oneLineTagged.append(('{}'.format(word), '{}'.format(posTag)))\n",
    "        taggedSentences.append(oneLineTagged)\n",
    "\n",
    "    return taggedSentences\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sentences to tuples i.e. ()\n",
    "taggedSents = transformToTuples(y, classes)\n",
    "\n",
    "# define a 75/25 train/test split\n",
    "cutoff = int(.75 * len(taggedSents))\n",
    "training_sentences = taggedSents[:cutoff]\n",
    "test_sentences = taggedSents[cutoff:]\n",
    "\n",
    "X, y = transform_to_dataset(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training beginning ---------------------------------------------\n",
      "Training completed in 15.8 seconds -----------------------------\n"
     ]
    }
   ],
   "source": [
    "# (pre optimization guess) \n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "\n",
    "print('Training beginning ---------------------------------------------')\n",
    "start = time.time() # timer\n",
    "clf.fit(X[:10000], y[:10000])   # Use only the first 10K samples if running it multiple times. It takes a fair while :)\n",
    "end = time.time()\n",
    "print('Training completed in {:.1f} seconds -----------------------------'.format((end - start)))\n",
    "\n",
    "X_test, y_test = transform_to_dataset(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipiline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ---------------------------------------------\n",
      "\n",
      "Training beginning ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siangk/miniconda3/envs/ml/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 18.5 seconds -----------------------------\n",
      "0.9301\n"
     ]
    }
   ],
   "source": [
    "print('RUNNING ---------------------------------------------')\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "# 1) create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('vec', DictVectorizer()),\n",
    "    ('clf', DummyEstimator())]) # placeholder\n",
    "\n",
    "# 2) define grid\n",
    "search_space = [\n",
    "    {'clf': [KNeighborsClassifier()],\n",
    "     'clf__n_neighbors': range(1,10),\n",
    "     'clf__weights': ['uniform', 'distance']\n",
    "     },\n",
    "     {'clf': [DecisionTreeClassifier()],\n",
    "      'clf__criterion': ['gini', 'entropy']\n",
    "     },\n",
    "     {'clf': [LogisticRegression()],\n",
    "      'clf__penalty': ['l1', 'l2'], \n",
    "      'clf__solver': ['saga'],#, 'newton-cg', 'lbfgs', 'liblinear', 'sag']#, \n",
    "      'clf__multi_class': ['ovr', 'multinomial', 'auto']\n",
    "     }\n",
    "]\n",
    "             \n",
    "# 3) conduct grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "\n",
    "# 4) fit and return accuracy & best params\n",
    "print('\\nTraining beginning ---------------------------------------------')\n",
    "start = time.time() # timer\n",
    "best_cv = clf.fit(X[:10000], y[:10000]) # Use only the first 10K samples if running it multiple times. It takes a fair while :)\n",
    "end = time.time()\n",
    "print('Training completed in {:.1f} seconds -----------------------------'.format((end - start)))\n",
    "\n",
    "# 5) return best \n",
    "best_model = best_cv.best_estimator_.get_params()['clf']\n",
    "\n",
    "# unsure what this is (?)\n",
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    "accuracy = best_cv.score(X_test, y_test)\n",
    "print(f\"{lr_housing_r2:,.4f}\")\n",
    "print(f\"best model: {best_cv.best_estimator_.get_params()['clf']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model: Logit<br>\n",
    "Best Param: <br>\n",
    "penalty = 'l2'<br>\n",
    "solver='saga'<br>\n",
    "Training completed in 226.1 seconds<br>\n",
    "Accuracy: 0.9304<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metric\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9301\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.4}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
