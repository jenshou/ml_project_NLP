{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization \n",
    "------------\n",
    "*Basic hyperparameter tuning to improve baseline performance*\n",
    "\n",
    "*Group Name: Destiny's Child*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Names\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Miguel Romero Calvo\n",
    "2. Jenny Kong\n",
    "3. Louise Lai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief description\n",
    "---\n",
    "Decision Trees are used to predict the POS tags for each word in a sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "import pprint \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35967 rows of data\n",
      "Here is a sample line:\n",
      "\t\"[16, 16, 9, 16, 15, 16, 16, 31, 16, 9, 7, 16, 35, 7, 15, 28, 30, 16, 2]\",Joint Chiefs of Staff Chairman Mike Mullen said Sunday that the U.S. has a plan to strike Iran .\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "with open('./train.csv') as train_file:\n",
    "    y = train_file.read().splitlines()\n",
    "    \n",
    "print(\"There are {} rows of data\".format(len(y)))\n",
    "print(\"Here is a sample line:\\n\\t{}\".format(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', ':', ';', 'CC', 'CD', 'DT', 'EX', 'IN', 'JJ', 'JJR', 'JJS', 'LRB', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'RRB', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '```']\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "with open('./classes.txt') as classes_file:\n",
    "    classes = classes_file.read().splitlines()\n",
    "\n",
    "print(list(classes))\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit scikit-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    " \n",
    "# pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2)) # example\n",
    "\n",
    "def transformToTuples(trainFile, classesFile):\n",
    "    classes # this should exist in global\n",
    "    taggedSentences = []\n",
    "    \n",
    "    # data cleaning\n",
    "    cleanedTrainFile = trainFile.copy()\n",
    "    del cleanedTrainFile[7561] # delete weird lines e.g.: [7],The\n",
    "    del cleanedTrainFile[10422] # [3],...\n",
    "    \n",
    "    i = 0\n",
    "    for i, line in enumerate(cleanedTrainFile):\n",
    "        splitLine = line.split(\"\\\",\")\n",
    "        \n",
    "        # extract the POS embeddings [0]\n",
    "        tagsString = splitLine[0].replace(\"\\\"\", \"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        tags = tagsString.split(\", \")\n",
    "        \n",
    "        # extract the sentence [1]\n",
    "        sentence = splitLine[1].strip(\"\\\"\") # from 2nd elem to second last, to remove the extra \"\n",
    "        \n",
    "        oneLineTagged = []\n",
    "        for tag, word in zip(tags, sentence.strip().split(\" \")):\n",
    "            #print(int(tag))\n",
    "            posTag = classes[int(tag)-1] \n",
    "            oneLineTagged.append(('{}'.format(word), '{}'.format(posTag)))\n",
    "        taggedSentences.append(oneLineTagged)\n",
    "\n",
    "    return taggedSentences\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sentences to tuples i.e. ()\n",
    "taggedSents = transformToTuples(y, classes)\n",
    "\n",
    "# define a 75/25 train/test split\n",
    "cutoff = int(.75 * len(taggedSents))\n",
    "training_sentences = taggedSents[:cutoff]\n",
    "test_sentences = taggedSents[cutoff:]\n",
    "\n",
    "X, y = transform_to_dataset(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training beginning ---------------------------------------------\n",
      "Training completed in 25.6 seconds -----------------------------\n"
     ]
    }
   ],
   "source": [
    "# (pre optimization guess) \n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "\n",
    "print('Training beginning ---------------------------------------------')\n",
    "start = time.time() # timer\n",
    "clf.fit(X[:10000], y[:10000])   # Use only the first 10K samples if running it multiple times. It takes a fair while :)\n",
    "end = time.time()\n",
    "print('Training completed in {:.1f} seconds -----------------------------'.format((end - start)))\n",
    "\n",
    "X_test, y_test = transform_to_dataset(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit multiple algorithms: ____\n",
    "\n",
    "    - At least one algorithm from class: ____\n",
    "\n",
    "- Fit multiple models for each algorithm: ____\n",
    "\n",
    "- Train-test split: ____\n",
    "\n",
    "- Hyperparameter tuning: ____\n",
    "\n",
    "- Regularization: ____\n",
    "\n",
    "- Pipelines: ____\n",
    "\n",
    "- Unnecessary use of random state=42 (lose points): ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [a for a in X[:50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING ---------------------------------------------\n",
      "\n",
      "Training beginning ---------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louiselai88gmail/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 529.0 seconds -----------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e6ff5e927017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# X_test, y_test = transform_to_dataset(test_sentences)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mlr_housing_r2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#################################Best Model: Logit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "print('RUNNING ---------------------------------------------')\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "# 1) create pipeline\n",
    "pipe = Pipeline([\n",
    "    ('vec', DictVectorizer()),\n",
    "    ('clf', DummyEstimator())]) # placeholder\n",
    "\n",
    "# 2) define grid\n",
    "search_space = [\n",
    "    {'clf': [KNeighborsClassifier()],\n",
    "     'clf__n_neighbors': range(1,10),\n",
    "     'clf__weights': ['uniform', 'distance']\n",
    "     },\n",
    "     {'clf': [DecisionTreeClassifier()],\n",
    "      'clf__criterion': ['gini', 'entropy']\n",
    "     },\n",
    "     {'clf': [LogisticRegression()],\n",
    "      'clf__penalty': ['l1', 'l2'], \n",
    "      'clf__solver': ['saga'],#, 'newton-cg', 'lbfgs', 'liblinear', 'sag']#, \n",
    "      'clf__multi_class': ['ovr', 'multinomial', 'auto']\n",
    "     }\n",
    "]\n",
    "             \n",
    "# 3) conduct grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "\n",
    "# 4) fit and return accuracy & best params\n",
    "print('\\nTraining beginning ---------------------------------------------')\n",
    "start = time.time() # timer\n",
    "best_cv = clf.fit(X[:10000], y[:10000]) # Use only the first 10K samples if running it multiple times. It takes a fair while :)\n",
    "end = time.time()\n",
    "print('Training completed in {:.1f} seconds -----------------------------'.format((end - start)))\n",
    "\n",
    "# 5) return best \n",
    "best_model = best_cv.best_estimator_.get_params()['clf']\n",
    "\n",
    "# unsure what this is (?)\n",
    "# X_test, y_test = transform_to_dataset(test_sentences)\n",
    "\n",
    "lr_housing_r2 = best_cv.score(X_test, y_test)\n",
    "\n",
    "#################################Best Model: Logit\n",
    "#################################Best Param: \n",
    "#################################penalty = 'l2'\n",
    "#################################solver='saga'\n",
    "# Training completed in 226.1 seconds -----------------------------\n",
    "# # 0.9304\n",
    "# best model: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "#           intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
    "#           n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
    "#           tol=0.0001, verbose=0, warm_start=False)\n",
    "############################################################################\n",
    "\n",
    "\n",
    "print(f\"{lr_housing_r2:,.4f}\")\n",
    "print(f\"best model: {best_cv.best_estimator_.get_params()['clf']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training beginning ---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# The best model\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "           intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
    "           n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
    "           tol=0.0001, verbose=0, warm_start=False))])\n",
    "\n",
    "print('Training beginning ---------------------------------------------')\n",
    "start = time.time() # timer\n",
    "clf.fit(X[:10000], y[:10000])   # Use only the first 10K samples if running it multiple times. It takes a fair while :)\n",
    "end = time.time()\n",
    "print('Training completed in {:.1f} seconds -----------------------------'.format((end - start)))\n",
    "\n",
    "X_test, y_test = transform_to_dataset(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metric\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-25313b35f6d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: {:.4}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \"\"\"\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             raise ValueError(\"No score function explicitly defined, \"\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_check_is_fitted\u001b[0;34m(self, method_name)\u001b[0m\n\u001b[1;32m    472\u001b[0m                                  % (type(self).__name__, method_name))\n\u001b[1;32m    473\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best_estimator_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_estimator_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {:.4}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
